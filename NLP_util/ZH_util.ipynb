{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#讀入我們會用到的套件\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import jieba\n",
    "from datetime import datetime\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True) #為了能在本地端調用\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import swifter\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True) #為了能在本地端調用\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "# Code Snippet for Creating LDA visualization\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pyLDAvis.gensim\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from textstat import flesch_reading_ease\n",
    "from hanziconv import HanziConv\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "jieba.load_userdict('/Users/Dennis/Python與商業分析/R_text_mining/my_dict.txt')\n",
    "\n",
    "\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file,'r' , encoding='gbk' , newline= '') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i.replace('\\r','') for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "\n",
    "stop_words_file = '/Users/Dennis/data_science/NLP /Term project/哈工大停用词表.txt'\n",
    "\n",
    "\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "new_stop_words = []\n",
    "for word in stopwords:\n",
    "    new_stop_words.append(HanziConv.toTraditional(word))\n",
    "\n",
    "    \n",
    "stopwords =  set(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n",
      "/Users/Dennis/anaconda3/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning:\n",
      "\n",
      "The html argument of XMLParser() is deprecated\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "      <th>from</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>◆外資連續5日買超個股</td>\n",
       "      <td>2018-01-04 07:33:07</td>\n",
       "      <td>外資連續 5 日買超個股\\n1. 外資連續買超 5 日的股票\\n(2891) 中信金、 (2...</td>\n",
       "      <td>anue</td>\n",
       "      <td>https://news.cnyes.com/news/id/4005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>新光金連3虧跌回後段班 15家金控前11月獲利3000億元差臨門一腳</td>\n",
       "      <td>2018-12-10 20:43:14</td>\n",
       "      <td>15 家上市櫃金控 11 月獲利今 (10) 日全數出爐，由於全球金融市場擺脫 10 月的肅...</td>\n",
       "      <td>anue</td>\n",
       "      <td>https://news.cnyes.com/news/id/4252670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>◆台灣集中市場投信買賣超前30名</td>\n",
       "      <td>2019-12-20 16:55:05</td>\n",
       "      <td>台北‧12月20日 交易日  \\n  \\n◆買超  \\n代碼  股票名稱  買超張數   收...</td>\n",
       "      <td>anue</td>\n",
       "      <td>https://news.cnyes.com/news/id/4425612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>◆投信連續3日買超個股</td>\n",
       "      <td>2019-09-12 20:53:09</td>\n",
       "      <td>投信連續 3 日買超個股\\n1. 投信連續買超 3 日的股票\\n(2884) 玉山金、 (3...</td>\n",
       "      <td>anue</td>\n",
       "      <td>https://news.cnyes.com/news/id/4381967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>玉山金啟動人才招募 跨界徵才380人</td>\n",
       "      <td>2014-12-27 10:23:30</td>\n",
       "      <td>玉山金(2884-TW)宣布啟動2015年人才招募計畫，廣招跨界人才，除了金融及商學背景外，...</td>\n",
       "      <td>anue</td>\n",
       "      <td>https://news.cnyes.com/news/id/1395217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title                time  \\\n",
       "0                         ◆外資連續5日買超個股 2018-01-04 07:33:07   \n",
       "1  新光金連3虧跌回後段班 15家金控前11月獲利3000億元差臨門一腳 2018-12-10 20:43:14   \n",
       "2                    ◆台灣集中市場投信買賣超前30名 2019-12-20 16:55:05   \n",
       "3                         ◆投信連續3日買超個股 2019-09-12 20:53:09   \n",
       "4                  玉山金啟動人才招募 跨界徵才380人 2014-12-27 10:23:30   \n",
       "\n",
       "                                             content  from  \\\n",
       "0  外資連續 5 日買超個股\\n1. 外資連續買超 5 日的股票\\n(2891) 中信金、 (2...  anue   \n",
       "1  15 家上市櫃金控 11 月獲利今 (10) 日全數出爐，由於全球金融市場擺脫 10 月的肅...  anue   \n",
       "2  台北‧12月20日 交易日  \\n  \\n◆買超  \\n代碼  股票名稱  買超張數   收...  anue   \n",
       "3  投信連續 3 日買超個股\\n1. 投信連續買超 3 日的股票\\n(2884) 玉山金、 (3...  anue   \n",
       "4  玉山金(2884-TW)宣布啟動2015年人才招募計畫，廣招跨界人才，除了金融及商學背景外，...  anue   \n",
       "\n",
       "                                     link  \n",
       "0  https://news.cnyes.com/news/id/4005407  \n",
       "1  https://news.cnyes.com/news/id/4252670  \n",
       "2  https://news.cnyes.com/news/id/4425612  \n",
       "3  https://news.cnyes.com/news/id/4381967  \n",
       "4  https://news.cnyes.com/news/id/1395217  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path  ='/Users/Dennis/data_science/Crawlers/finance_news_crawler/玉山財經新聞.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀入我們會用到的套件\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import jieba\n",
    "from datetime import datetime\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True) #為了能在本地端調用\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import swifter\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True) #為了能在本地端調用\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "# Code Snippet for Creating LDA visualization\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pyLDAvis.gensim\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from textstat import flesch_reading_ease\n",
    "from hanziconv import HanziConv\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "jieba.load_userdict('/Users/Dennis/Python與商業分析/R_text_mining/my_dict.txt')\n",
    "\n",
    "\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file,'r' , encoding='gbk' , newline= '') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i.replace('\\r','') for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "\n",
    "stop_words_file = '/Users/Dennis/data_science/NLP /Term project/哈工大停用词表.txt'\n",
    "\n",
    "\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "new_stop_words = []\n",
    "for word in stopwords:\n",
    "    new_stop_words.append(HanziConv.toTraditional(word))\n",
    "\n",
    "    \n",
    "stopwords =  set(new_stop_words)\n",
    "# 前處理函式\n",
    "def generate_line_sentence(all_text):  \n",
    "    # 以句號分句\n",
    "    all_text = re.split('[。，！？!?,]', all_text)\n",
    "    \n",
    "    # 去除所有非中文字元\n",
    "#     all_text = [_get_chinese(s) for s in all_text]\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "def train_lda_objects(text , topic_num , stop):\n",
    "#     nltk.download('stopwords')    \n",
    "    \n",
    "    \n",
    "    def _preprocess_text(text):\n",
    "        \n",
    "        corpus=[]\n",
    "        stem=PorterStemmer()\n",
    "        lem=WordNetLemmatizer()\n",
    "        for news in text:\n",
    "            words=[w for w in word_tokenize(news) if (w not in stop)]\n",
    "\n",
    "            words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "\n",
    "            corpus.append(words)\n",
    "        return corpus\n",
    "    \n",
    "    corpus=_preprocess_text(text)\n",
    "    \n",
    "    dic=gensim.corpora.Dictionary(corpus)\n",
    "    bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "    \n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = topic_num , \n",
    "                                   id2word = dic,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "    \n",
    "    return lda_model, bow_corpus, dic\n",
    "\n",
    "def plot_lda_vis(lda_model, bow_corpus, dic):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
    "    pyLDAvis.show(vis)\n",
    "    return vis\n",
    "\n",
    "\n",
    "class NLP_ZH_Assistant:\n",
    "    \"\"\"\n",
    "    @method\n",
    "    \n",
    "    1. show% : NLP EDA assistant \n",
    "    \n",
    "    2. update% : adjust attr in this class\n",
    "    \n",
    "    3. ml% : machine learning algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self , df , text , stopwords ,big_category):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            df(DataFrame) : data\n",
    "\n",
    "            text(str): text feature name\n",
    "                \n",
    "            stopwords(set) : stopwords for clean text \n",
    "\n",
    "            big_category(str) : 最大類別的變數\n",
    "        \n",
    "        Example:\n",
    "            (df , 'text' , set , 'channel')\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "        self.text = text\n",
    "        \n",
    "        #Text series\n",
    "        self.content  = df[text]\n",
    "        def chinese_word_cut(mytext):\n",
    "            try:\n",
    "                word = \" \".join(jieba.cut(mytext))\n",
    "            except:\n",
    "                word = ''\n",
    "            return word\n",
    "        self.big_category = big_category\n",
    "        corpus = df[text].apply(lambda x: chinese_word_cut(x))\n",
    "        self.corpus = corpus\n",
    "        df['cut_text'] = corpus.apply(lambda x:x.split())\n",
    "        self.df = df\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        self.top_word = None\n",
    "        self.stopwords = stopwords\n",
    "        self.words_cooc_matrix = None\n",
    "#         self.stopwords = set(stopwords.words('english'))\n",
    "        all_content = []\n",
    "        for i in list(df[text]):\n",
    "            all_content.append({'content':str(i)})\n",
    "        self.all_content = all_content\n",
    "\n",
    "    \n",
    "    def show_each_text_length_dist(self):\n",
    "        self.content.str.len().hist()\n",
    "        plt.title('The number of characters present in each sentence')\n",
    "        plt.show()\n",
    "    \n",
    "    def show_each_text_used_word_dist(self):\n",
    "        \n",
    "        self.df[self.text].str.split().\\\n",
    "        map(lambda x: len(x)).\\\n",
    "        hist()\n",
    "        plt.title('The number of words appearing in each news headline.')\n",
    "        plt.show()\n",
    "    \n",
    "    def show_average_word_length_in_each_sentence(self): \n",
    "        print(' Stopwords are the words that are most commonly used in any language such as “the”,” a”,” an” etc. As these words are probably small in length these words may have caused the above graph to be left-skewed.')\n",
    "        self.df[self.text].str.split().\\\n",
    "           apply(lambda x : [len(i) for i in x]). \\\n",
    "           map(lambda x: np.mean(x)).hist()\n",
    "        plt.title('The average word length in each sentence.')\n",
    "        plt.show()\n",
    "    \n",
    "    def show_top_stopwords_bar_plot(self):\n",
    "        text = self.df[self.text]\n",
    "        stop= self.stopwords\n",
    "\n",
    "        \n",
    "        #Store it into our corpus\n",
    "        corpus  = self.corpus \n",
    "        from collections import defaultdict\n",
    "        dic=defaultdict(int)\n",
    "        for word in corpus:\n",
    "            if word in stop:\n",
    "                dic[word]+=1\n",
    "        \n",
    "        top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "        x,y=zip(*top)\n",
    "        plt.bar(x,y)\n",
    "        plt.title('Top_stopwords_bar_plot')\n",
    "        plt.show()\n",
    "        \n",
    "    def show_word_freq_except_to_stopwords(self , first_num):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            \n",
    "            first_num(int): How many first common words you want\n",
    "        \"\"\"\n",
    "        counter=Counter(np.sum(self.df.cut_text))\n",
    "        stop=self.stopwords\n",
    "        most=counter.most_common()\n",
    "        x, y= [], []\n",
    "        for word,count in most[:first_num]:\n",
    "            if (word not in stop):\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.title('Word Frequency in our text ')\n",
    "        sns.barplot(x=y,y=x)\n",
    "        plt.show()\n",
    "        \n",
    "    def show_top_ngram(self, n=None , first_num = 10):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            n(int) : N-gram params\n",
    "            first_num(int) : How many first common words you want\n",
    "        \"\"\"\n",
    "        text = self.text\n",
    "        df = self.df\n",
    "        \n",
    "        corpus = self.corpus\n",
    "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "        bag_of_words = vec.transform(df[text])\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) \n",
    "                      for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_n_bigrams= words_freq[:first_num]\n",
    "        x,y=map(list,zip(*top_n_bigrams))\n",
    "        sns.barplot(x=y,y=x)\n",
    "        freq_n_gram = set(sum([x.split() for x in pd.DataFrame(top_n_bigrams)[0]],[]))\n",
    "        return list(freq_n_gram)\n",
    "    \n",
    "    def clean_text_except_chinese(self):\n",
    "        \n",
    "        content = self.content\n",
    "        stop_words = self.stopwords\n",
    "        punctuations = [\"《\", \"》\", \"【\", \"】\", \"｜\", \"(\",\")\",  \"®\", \"\\n\", \"？\", \"@\", \"#\", \"?\", \"！\", \"!\" ,\"，\" , ',' , '►' ,'\\n']\n",
    "\n",
    "\n",
    "        \n",
    "        print('移除掉標點符號、數字、英文')\n",
    "        def remove_english(words):\n",
    "            new_word = ' '\n",
    "            for string in words:\n",
    "                if not re.search(r'[a-zA-Z]', string):\n",
    "                    new_word += string\n",
    "                else:\n",
    "                    new_word += ' '\n",
    "            return new_word\n",
    "        for word in stop_words:\n",
    "        #空白來抽換掉\n",
    "            content = content.replace(word , ' ')\n",
    "        \n",
    "            #移除標點符號\n",
    "        for punc in punctuations:\n",
    "            content = content.replace(punc , ' ')\n",
    "            #移除數字\n",
    "        new_content = \"\"\n",
    "        for word in content:\n",
    "            if word.isdigit() :\n",
    "                new_content += ' '\n",
    "            else:\n",
    "                new_content += word\n",
    "        #移除掉網址等英文字\n",
    "        new_content = remove_english(new_content)\n",
    "\n",
    "        self.content = new_content\n",
    "    def create_tidy_text(self , group):\n",
    "        \"\"\"\n",
    "        創建tidy text 架構的列表\n",
    "        Input\n",
    "            group(str) : 其次分類類別的變數\n",
    "         \n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        corpus = self.corpus\n",
    "        big_category = self.big_category\n",
    "        \n",
    "        \n",
    "        tidy_text = df.groupby([big_category , group])['cut_text'].sum().apply(pd.Series).stack().reset_index()\n",
    "\n",
    "        #更改column名稱\n",
    "        tidy_text = tidy_text.rename(columns = {0 :'segment'})\n",
    "\n",
    "        #將長度<=1 的去掉\n",
    "        long_segment = tidy_text.segment.apply(lambda seg : True if len(seg) > 1 else False )\n",
    "\n",
    "        tidy_text = tidy_text[long_segment].reset_index(drop = True)\n",
    "        display(tidy_text.head())\n",
    "        tidy_text['word_count'] = 1\n",
    "        group_df = pd.DataFrame(tidy_text.groupby([big_category , 'segment']).sum())\n",
    "        top_word = pd.DataFrame(group_df['word_count'].groupby(level=0, group_keys=False).nlargest(10))\n",
    "    \n",
    "        #轉換為Tidy structure\n",
    "        top_word = top_word.reset_index()\n",
    "        self.top_word = top_word\n",
    "        \n",
    "    def show_word_freq_ranking(self):\n",
    "        \"\"\"詞頻分析\"\"\"\n",
    "        top_word = self.top_word\n",
    "        big_category = self.big_category\n",
    "        \n",
    "        \n",
    "        different_youtubers = df[big_category].unique()\n",
    "        youtuber_num = len(different_youtubers)\n",
    "\n",
    "\n",
    "        drop_str = []\n",
    "        for youtuber in different_youtubers:\n",
    "            #這個youtuber的詞頻\n",
    "            plt.figure(figsize = (6,4))\n",
    "            this_youtuber_word = top_word[top_word[big_category] == youtuber]\n",
    "            for seg in this_youtuber_word.segment:\n",
    "                drop_str.append(seg)\n",
    "            plt.barh(this_youtuber_word.segment , this_youtuber_word.word_count)\n",
    "            plt.title(youtuber)\n",
    "            plt.yticks()\n",
    "        \n",
    "    def show_TF_IDF_ranking(self):\n",
    "        import jieba.analyse\n",
    "        top_word = self.top_word\n",
    "        youtubers = top_word.groupby(self.big_category).segment.sum()\n",
    "\n",
    "\n",
    "        for channel,text in zip(youtubers.to_frame().index , youtubers):\n",
    "            plt.figure(figsize = (6,4))\n",
    "            plt.title(channel)\n",
    "            message = pd.DataFrame(jieba.analyse.extract_tags(text , topK= 10,withWeight=True))\n",
    "            plt.barh(message[0] , message[1])\n",
    "    \n",
    "    def create_occr_matrix(self  ,allowed_words):\n",
    "         \n",
    "        #把每個youtuber的前三名取出來\n",
    "        #準備好我們的文件\n",
    "        documents = []\n",
    "        for content in df['cut_text']:\n",
    "            documents.append(content)\n",
    "        def create_co_occurences_matrix(allowed_words, documents):\n",
    "            \"\"\"\n",
    "            Input:\n",
    "                allowed_words:單維列表，也就是希望觀察的詞彙兩兩間關係\n",
    "                documents:二維列表\n",
    "            Output:\n",
    "                value_matrix : 詞與詞的共同出現頻率\n",
    "            \"\"\"\n",
    "            word_to_id = dict(zip(allowed_words, range(len(allowed_words))))\n",
    "            documents_as_ids = [np.sort([word_to_id[w] for w in doc if w in word_to_id]).astype('uint32') for doc in documents]\n",
    "            row_ind, col_ind = zip(*itertools.chain(*[[(i, w) for w in doc] for i, doc in enumerate(documents_as_ids)]))\n",
    "            data = np.ones(len(row_ind), dtype='uint32')  # use unsigned int for better memory utilization\n",
    "            max_word_id = max(itertools.chain(*documents_as_ids)) + 1\n",
    "            docs_words_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(len(documents_as_ids), max_word_id))  # efficient arithmetic operations with CSR * CSR\n",
    "            words_cooc_matrix = docs_words_matrix.T * docs_words_matrix  # multiplying docs_words_matrix with its transpose matrix would generate the co-occurences matrix\n",
    "            words_cooc_matrix.setdiag(0)\n",
    "        #     print(f\"words_cooc_matrix:\\n{words_cooc_matrix.todense()}\")\n",
    "            return words_cooc_matrix.todense(), word_to_id \n",
    "        words_cooc_matrix , word_to_id =  create_co_occurences_matrix(allowed_words , documents)\n",
    "        words_cooc_matrix =  pd.DataFrame(words_cooc_matrix)\n",
    "        words_cooc_matrix.index = allowed_words\n",
    "        words_cooc_matrix.columns = allowed_words\n",
    "        self.words_cooc_matrix = words_cooc_matrix\n",
    "        return words_cooc_matrix\n",
    "    \n",
    "    def show_co_occurence_structure(self,heat = True,  graph = True):\n",
    "        \"\"\"共現矩陣視覺化\"\"\"\n",
    "        if heat:\n",
    "            plt.figure(figsize = (16,9))\n",
    "            sns.heatmap(self.words_cooc_matrix, cmap=\"YlGnBu\")\n",
    "        \n",
    "        if graph:\n",
    "            plt.figure(figsize = (16,10))\n",
    "            #從matrix轉成圖\n",
    "            G =  nx.from_pandas_adjacency(self.words_cooc_matrix)\n",
    "            # Graph with Custom nodes:\n",
    "            nx.draw(G, with_labels=True, node_size=100, node_color=\"skyblue\", node_shape=\"s\", alpha=0.5, linewidths=40)\n",
    "            plt.show()\n",
    "        \n",
    "    def show_word_cloud(self , font_path):\n",
    "        \"\"\"\n",
    "        文字雲視覺化\n",
    "        \"\"\"\n",
    "        top_word = self.top_word\n",
    "        top_word_dict = dict()\n",
    "        for segment , _count in zip(top_word.segment , top_word.word_count):\n",
    "            top_word_dict[segment] = _count \n",
    "            \n",
    "        from wordcloud import WordCloud\n",
    "        cloud = WordCloud(font_path = font_path ).generate_from_frequencies(top_word_dict)\n",
    "        plt.figure()\n",
    "        plt.figure( figsize=(16,8), facecolor='k')\n",
    "        plt.imshow(cloud,interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        #顯示用\n",
    "        plt.show()\n",
    "        \n",
    "        # 接收語料(dict)與關鍵詞(string)，回傳語料當中關鍵詞出現的句子中，按照頻率排列的動詞表\n",
    "    def humanities_generate_verb_list(self , word):\n",
    "#         try:\n",
    "        raw_text_list = self.all_content\n",
    "        a = [generate_line_sentence(article['content']) for article in raw_text_list]\n",
    "        l = [sen for article in a for sen in article if word in sen]\n",
    "        l = [list(pseg.cut(ll)) for ll in l]\n",
    "        wordlist = [word for sen in l for (word, pos) in sen if pos.startswith('v')]\n",
    "        freq_dict = {w:wordlist.count(w) for w in set(wordlist)}\n",
    "        verb_df = sorted(freq_dict.items(), key=lambda kv: -kv[1])\n",
    "        temp_df = pd.DataFrame(verb_df)\n",
    "        s = pd.Series(temp_df[1])\n",
    "        s.index = temp_df[0]\n",
    "        display(s.head())\n",
    "        s = pd.DataFrame(s)\n",
    "        s.columns = ['count']\n",
    "        s.query('count > 5').iplot(kind ='bar')\n",
    "        return sorted(freq_dict.items(), key=lambda kv: -kv[1])\n",
    "#         except:\n",
    "#             print('語料庫中不存在此單詞')\n",
    "\n",
    "    # 接收語料(dict)與關鍵詞(string)，回傳語料中含有該關鍵詞的concordance dataframe\n",
    "    def humanities_generate_concordance_df(self ,  word):\n",
    "        try:\n",
    "            raw_text_list = self.all_content\n",
    "            a = [generate_line_sentence(article['content']) for article in raw_text_list]\n",
    "            l = [sen.split(word) for article in a for sen in article if word in sen]\n",
    "            df = pd.DataFrame({\n",
    "            \"left_context\": [s[0] for s in l],\n",
    "            \"keyword\": word,\n",
    "            \"right_context\": [s[1] for s in l],\n",
    "            })\n",
    "            return df\n",
    "        except:\n",
    "            print('語料庫中不存在此單詞')\n",
    "            \n",
    "   \n",
    "\n",
    "    def ml_lda_script(self , topic_num , show_lda_vis = False):\n",
    "        \"\"\"\n",
    "        LDA modeling\n",
    "        \"\"\"\n",
    "        text = self.content\n",
    "        print('Start LDA modeling...')\n",
    "        lda_model, bow_corpus, dic = train_lda_objects(text , topic_num , self.stopwords)\n",
    "        word_dict = {};\n",
    "        for i in range(topic_num):\n",
    "            #先列出每個主題的前20個關鍵詞\n",
    "            words = lda_model.show_topic(i, topn = 20);\n",
    "            word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "       \n",
    "        display( pd.DataFrame(word_dict))\n",
    "        \n",
    "        if show_lda_vis:\n",
    "            print('Dashboard printing...')\n",
    "            plot_lda_vis(lda_model, bow_corpus, dic)\n",
    "            \n",
    "\n",
    "font_path = '/Users/Dennis/ttf/SimHei 拷貝.ttf'\n",
    "tool = NLP_ZH_Assistant(df , 'content' , stopwords = stopwords , big_category='from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "line": {
          "color": "rgba(255, 153, 51, 1.0)",
          "dash": "solid",
          "shape": "linear",
          "width": 1.3
         },
         "mode": "lines",
         "name": "title",
         "text": "",
         "type": "scatter",
         "uid": "adb39ed5-0c19-48ba-b2a1-46b8f816ad7c",
         "x": [
          "Newtalk",
          "NowNews",
          "anue",
          "appledaily",
          "chinatimes",
          "ettoday",
          "ftv",
          "setn",
          "storm",
          "technews",
          "tvbs"
         ],
         "y": [
          227,
          222,
          96,
          2,
          100,
          50,
          18,
          12,
          33,
          20,
          71
         ]
        }
       ],
       "layout": {
        "legend": {
         "bgcolor": "#F5F6F9",
         "font": {
          "color": "#4D5663"
         }
        },
        "paper_bgcolor": "#F5F6F9",
        "plot_bgcolor": "#F5F6F9",
        "title": {
         "font": {
          "color": "#4D5663"
         }
        },
        "xaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": ""
         },
         "zerolinecolor": "#E1E5ED"
        },
        "yaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": ""
         },
         "zerolinecolor": "#E1E5ED"
        }
       }
      },
      "text/html": [
       "<div id=\"589381d0-544f-4e0b-bd32-af4d35a30d4d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\")) {\n",
       "    Plotly.newPlot(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\", [{\"line\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"dash\": \"solid\", \"shape\": \"linear\", \"width\": 1.3}, \"mode\": \"lines\", \"name\": \"title\", \"text\": \"\", \"x\": [\"Newtalk\", \"NowNews\", \"anue\", \"appledaily\", \"chinatimes\", \"ettoday\", \"ftv\", \"setn\", \"storm\", \"technews\", \"tvbs\"], \"y\": [227, 222, 96, 2, 100, 50, 18, 12, 33, 20, 71], \"type\": \"scatter\", \"uid\": \"4bab7c0e-d7d4-4e9b-972d-7ad5f20d8810\"}], {\"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"title\": {\"font\": {\"color\": \"#4D5663\"}}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"text\": \"\", \"font\": {\"color\": \"#4D5663\"}}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"text\": \"\", \"font\": {\"color\": \"#4D5663\"}}, \"zerolinecolor\": \"#E1E5ED\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\")) {window._Plotly.Plots.resize(document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"589381d0-544f-4e0b-bd32-af4d35a30d4d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\")) {\n",
       "    Plotly.newPlot(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\", [{\"line\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"dash\": \"solid\", \"shape\": \"linear\", \"width\": 1.3}, \"mode\": \"lines\", \"name\": \"title\", \"text\": \"\", \"x\": [\"Newtalk\", \"NowNews\", \"anue\", \"appledaily\", \"chinatimes\", \"ettoday\", \"ftv\", \"setn\", \"storm\", \"technews\", \"tvbs\"], \"y\": [227, 222, 96, 2, 100, 50, 18, 12, 33, 20, 71], \"type\": \"scatter\", \"uid\": \"4bab7c0e-d7d4-4e9b-972d-7ad5f20d8810\"}], {\"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"title\": {\"font\": {\"color\": \"#4D5663\"}}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"text\": \"\", \"font\": {\"color\": \"#4D5663\"}}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"text\": \"\", \"font\": {\"color\": \"#4D5663\"}}, \"zerolinecolor\": \"#E1E5ED\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\")) {window._Plotly.Plots.resize(document.getElementById(\"589381d0-544f-4e0b-bd32-af4d35a30d4d\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('from')['title'].count().iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
